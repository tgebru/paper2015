\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Visual Census Using Cars}

\author{First Author\\
Institution1\\
Institution1 address\\
{\tt\small firstauthor@i1.org}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
\and
Second Author\\
Institution2\\
First line of institution2 address\\
{\tt\small secondauthor@i2.org}
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
Detecting a large number of BMWs in images informs us that those images may be of a wealthy area. Conversely, knowing that our images were obtained from a wealthy neighborhood increases the likelihood of detecting expensive cars. We explore this relationship between demographic factors and fine-grained classes by performing large scale detection of over 2600 car classes and conducting a social analysis of unprecedented scale in computer vision. Using 45 million images from 200 of the biggest cities in the United States, we predict demographic factors such as neighborhood wealth and education levels. Finally we show that just as fine-grained classes provide demographic information, societal cues can assist in fine-grained classification and improve accuracy. To facilitate our work, we have collected the largest and most challenging fine-grained dataset reported to date consisting of 3147 classes of cars comprised of images from google street view and other web sources and classified by car experts to account for even the most subtle of visual differences. We hope our work ushers in a new research area fusing fine-grained object detection and societal analysis.

\end{abstract}

\section{Introduction}
%%%%%%%%% BODY TEXT
The artifacts with which we choose to surround ourselves tell us not only about ourselves but also about the society in which we live. In the 21st century, some of the most relevant objects defining people and their lifestyles are houses, clothes and cars. From a single picture such as fig.~\ref{fig:pull} of an individual with their car, we can guess that the person is rich because they own a tesla, that they probably care about the environment and live in a mostly liberal and wealthy area such as San Francisco. Traditionally, the most prevalent method for gathering such personal information is through surveys: in 2010, the US government spent XXX on the census and American community survey (ACS) projects. However, the emergence of big and diverse sets of data generated by people has enabled computer scientists and computational sociologists to gain interesting societal insights by analyzing massive user texts and social networks~\cite{jure}~\cite{nlp_people}. Although there has been no work in computer vision matching the depth of analysis by works such as ~\cite{jure}~\cite{nlp_people}, a few pioneering works by Torralba etal, Berg etal, and Hedalgo etal have recently started to apply visual scene analysis techniques to infer characteristics of neighborhoods and cities~\cite{antonio}~\cite{mcdonalds}~\cite{mit_cvpr}~\cite{tamara}. All of the computer vision techniques used in these works use global scene level features to analyze cities.

\begin{figure}[t]
\begin{center}
   \includegraphics[width=1\linewidth]{img/pull.jpg}
\end{center}
   \caption{some pull figure}
\label{fig:pull}
\end{figure}

\label{fig:dataset1}
\begin{figure*}[t]
\begin{center}
   \includegraphics[width=0.435\linewidth]{img/web.png}
   \includegraphics[width=0.475\linewidth]{img/streetview.png}
\end{center}
   \caption{Left: examples of cars from craigslist, cars.com and edmunds.com, Right: examples of cars from streetview images. Cars from streetview images are blurred and occluded.}
\end{figure*}

In this work, we are also interested in analyzing cities and neighborhoods and the demographic makeup of their inhabitants. However, our approach uses fine-grained object recognition as opposed to global scene features. In particular, we are interested in using cars as a lens into understanding our society. Fig.~\ref{fig:pull} shows that cars say a lot about us, and XX\% of Americans own cars. Furthermore, due to the ubiquity of geotagged images such as street view, cars are more visible and easier to obtain images for compared to more personal items. Thus, to facilitate our work, we gathered a dataset for 3147 car types consisting of all cars produced after 1990. Our dataset consists of labeled cars for training car detection and classification models, mined from specialty websites such as edmunds.com, cars.com and craigslist as well as street view images. In order to perform societal analysis, we obtained more than 45 million google street view images from 200 American cities. One might ask if street view car detections can tell as an accurate story of our neighborhoods and cities, since after all, these pictures were taken at different times, and cars are mobile objects. Our analysis in Section~\ref{sec:social} shows that surprisingly, results using these detections correlated highly with demographic data obtained from ACS as well as government sources. This work is only possible because of the tremendous progress in object recognition: particularly object detection and fine-grained object recognition. We use DPM~\cite{dpm} to detect cars due to scalability concerns and train a CNN~\cite{alexnet} to classify them into one of 2567 classes. Section~\ref{sec:detection} discusses the tradeoffs we considered for car detection and classification. Using the result of our detections, we are able to offer insights into a number of very interesting questions related to our lives, communities and cities. Section~\ref{sec:social} shows a series of analyses about how the types of cars we drive relate to our level of education, wealth and the neighborhoods and cities we live in. Finally, section~\ref{sec:prior} explores the use of social knowledge such as median household income in a zip code to improve fine-grained car classification. 

  \begin{enumerate}
    \item We present the largest fine-grain car dataset reported to our knowledge.
    \item We perform mass fine-grained detection across 200 cities 
    \item We perform a large scale social analysis of cities using street view car detections.
  \end{enumerate}

%------------------------------------------------------------------------
\section{Related Work}
\textbf{City analysis via image features}. There has been recent interest in using images to characterize cities~\cite{mit_plos_1}~\cite{tamara}~\cite{paris}~\cite{antonio}~\cite{mit_cvpr}~\cite{mcdonalds}. ~\cite{mit_plos_1} created scores for perceptions of wealth, safety and uniqueness by asking people to rate images from 3 cities on a scale of 1\textendash10 and ~\cite{mit_cvpr}~\cite{tamara} predicted these scores using various global image features such as GIST and CNN. In another line of work analyzing cities, ~\cite{antonio} perform city identity recognition after representing each city with higher level attributes. ~\cite{paris} identify unique qualities of cities such as Paris and Prague and~\cite{mcdonalds} shows that given an image of a particular city location, it is possible to predict the most likely direction for the location of a McDonalds. While our work also shares the motive of city analysis through imagery, it differs in that we use fine-grained object recognition to achieve this goal. As shown in section~\ref{sec:social}, this allows us to perform much more extensive social and demographic analysis through imagery and also easily extend our analysis to many other cities where as~\cite{tamara}, for example, showed that their method is not easily generalizable to other cities (e.g. training on Boston images does not give good predictions for San Francisco).\newline\newline
\textbf{Fine-grained object recognition}. Fine-grained object recognition is a difficult problem due to the high visual similarity between classes. Nevertheless, recent works such as ~\cite{ning}~\cite{steve}~\cite{jon} show impressive results by using part annotated datasets such as ~\cite{birds}~\cite{dogs}~\cite{cars} and augmenting state of the art object detection algorithms such as RCNN~\cite{rcnn}. However, it is difficult to evaluate fine-grained classification accuracy since there are no fine-grained datasets that match object classification datasets like imagenet~\cite{imagenet} in the number of classes or images. Recent works such as~\cite{birdsnap} have introduced larger scale fine-grained datasets and ~\cite{nyc3d} has introduced a 3D car dataset annotated with metadata such as location information. We introduce a geotagged car dataset with unprecedented scale in both the number of classes and images.\newline\newline
\textbf{Using GPS data to improve classification}. Although an increasing number of images that we interact with daily are associated with GPS tags, there are very few computer vision algorithms that take advantage of location based metadata. However, recent works such as~\cite{amir} use location information to assist in detecting objects such as trash cans and street lamps,~\cite{birdsnap} learns a spatio-temporal prior to improve bird classification and~\cite{nyc3d} uses some location information such as elevation to assist in car classification. 

%------------------------------------------------------------------------
\begin{figure} [t]
\begin{center}
\includegraphics[width=0.8\linewidth]{img/car_hierarchy.jpg}
\end{center}
\caption {Car class hierarchy. Classes are usually more visually similar when we travel down the tree.}
\label{fig:hierarchy}
\end{figure}

\begin{figure} [t]
\begin{center}
\includegraphics[width=0.45\linewidth]{img/web.jpg}
\includegraphics[width=0.45\linewidth]{img/street.jpg}
\end{center}
\caption {Distribution of images for web and street view classes.}
\label{fig:img_dist}
\end{figure}

\section{Data}

We first collected 45 million images from 8 million GPS points by sampling the roads of 200 of the biggest US cities every 25m. For each GPS coordinate we gather street view images at 0, 60, 120, 180, 240, 300 degree rotations. In order to train a fine-grained car classifier to detect and classify the cars in our gathered images, we created the largest ever reported fine-grained dataset of cars consisting of all the cars listed on edmunds.com (all cars manufactured after 1990). In order to gather this dataset, we first created a class list using edmunds.com by obtaining example images of all the cars they have (18017 cars) and grouped the cars into visually indistinguishable classes using a series of amazon mechanical turk tasks as well as manual labor by the authors. This resulted in 3147 visually indistinguishable classes. We then collected additional images of cars from cars.com and craigslist and used AMT to obtain 313,099 bounding boxes of cars for these images and 543,641 bounding boxes for our street view images.

Finally, we hired car experts to label 69,492 bounding boxes of cars from the street view images, classifying them into one of the 3147 classes that we created. We also labeled cars from craigslist and cars.com by parsing the car posting titles. The final car dataset consists of 781,922 images where 468,823 are from street view and 313,099 are images from craigslist.com, cars.com and edmunds.com. Our training, validation and test sets roughly consist of 50\%, 10\% and 40\% of our street view images and images from all other sources are also included in the training set. Although we are interested in classifying street view images, having car images from other sources in our training set increases the amount of data we have without paying expensive car experts to annotate street view images. Fig.~\ref{fig:dataset1} shows some examples of cars from our dataset. We can see that cars from street view images are generally occluded and blurry where as cars from other web sources have higher resolution and are not occluded.

Fig.~\ref{fig:hierarchy} shows one of a few possible car hierarchies. The leaves are the fine-grained classes and the parents are composite classes consisting of the children. Cars become more and more visually indistinguishable while traveling down the tree, with the most similar classes, in most cases, being the leaves sharing a parent. As the figure shows, the difference between class 1 and class 2 is extremely subtle. Given the difficulty of our dataset, we were very surprised by the 31.27\% accuracy of our classifier at the leaf level. 

\section{Detecting and classifying cars}
\label{sec:detection}
Although RCNN based fine-grained detection algorithms have reported state of the art results, ~\cite{rcnn}~\cite{ning}~\cite{branson}, its computational and memory requirements make it impractical for use in a large scale detection setting such as ours. Training an RCNN detector on our data would require \(\sim\)XXGB of memory and XXX secs and detecting cars  would take \(\sim\)20s per image and XXX days for \(\sim\)45 million images. Thus our pipeline, instead, consists of using DPM~\cite{dpm} to detect cars and a CNN ~\cite{alexnet} to classify them. We present details in the sections below.

\subsection{Car Detection}
Inorder to evaluate accuracy/speed trade off, we trained DPMS with different numbers of components and parts and used the model with the best tradeoff for detection. Our final algorithm uses a single component 8 part DPM and achieves an AP of 64.2\% while taking 5 secs per image. As a point of comparison, the highest AP (68.7\%) was achieved with a 5 component 8 part DPM which takes \(\sim\)22 secs per image. Detailed plots and timing measurements of other DPMs we have trained are discussed in our supplementary material.


\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Attribute} & \textbf{Acc\%} \\
\hline\hline
Make & X \\
Model & X \\
Submodel & X \\
Year (5 bins) & X \\
Price (5 bins)& X \\
Domestic/Foreign & X\\
Country & X\\
\hline
\end{tabular}
\end{center}
\caption{Accuracy by car attribute}
\label{table:tree-acc}
\end{table}

\begin{figure*} [t]
\begin{center}
\includegraphics[width=0.55\linewidth]{img/good_det.png}
\raisebox{-0.02\height}
 {
\includegraphics[width=0.34\linewidth]{img/cars.jpg}
 }
\end{center}
\caption {An example of our car detections and classifications. The red boxes are DPM bounding boxes that are not classified and green boxes are classifications. The two cars in the image are correctly classified to the classes shown on the right.}
\label{fig:dets}
\end{figure*}

\subsection{Car Classification}
We use a standard CNN from ~\cite{alexnet} with ~\cite{caffe} to classify the DPM detections into one of 2657 fine-grained classes. Although our test set consists of street view images, as mentioned in sec.~\ref{sec:dataset} 61\% of our positive training images are composed of cars from other sources such as craigslist. Thus we add deformations such as blurring to make the non-street view images look like street view images. We give details of training the CNN in our supplementary material. At test time we take the top 10\% scoring DPM bounding boxes and classify them. This results in a 10x increase in speed but only a 0.X\% drop in AP as compared to using all the boxes detected by DPM. We achieve an accuracy of 33.X\% on the true positive DPM bounding boxes and 31.27\% on the ground truth bounding boxes. Fig.~\ref{fig:good_dets} shows an example of our street view detections. The cars are detected and classified correctly even though blurriness and occlusions in street view images makes this a difficult task.

\subsection{Analyzing Hierarchical Classification Accuracy}
Some types of classification mistakes are more costly than others for the task of social analysis. For example, an error misclassifying 2001 Honda Accord lx to 2001 Honda Accord dx is not significant. However, misclassifying a 2012 BMW 3-series to a 1996 Honda Accord would create large errors in an analysis measuring the realtionship between the average car price or age in a zip code and median household income. Similarly, misclassifying an SUV to a sedan would confound an analysis estimating the cities with the most number of large cars. In order to gain more insight into the types of errors our classifier makes, we measure the accuracy of classifying different car attributes. Table~\ref{table:att-acc} lists accuracies by various car attributes such as those in fig.~\ref{fig:hierarchy} and others like car price, year etc\ldots We can see that the accuracy is much higher after aggregating by different attributes.  



 \section{Societal analysis}
\label{sec:social}
In this section, we present societal analysis results from all 200 cities as well as case studies from those with available ground truth data. We divide our analysis into different sections below.

\subsection{What cars on the street tell us about people}

\subsubsection{Sanity Check}
The first question we asked is how do cars on the street relate to the cars that people drive? Specifically,
can we learn about the registered cars in a zip code from our street view detections?
We downloaded vehicle census data from Massachusetts, which is the only state to release extensive vehicle registration data, and found an extremely high Pearson correlation coefficient of 0.9 (p value=XXX) between the number of cars we detected per zip code and the number of cars that are registered. The high correlation was only obtained after aggregating cars at the zip code level which shows that most people in MA drive within their zip code. 
After establishing a high correlation between the number of cars we detect and the number of registered cars, we also measured the correlation between the make of the detected and registered cars per zip code. As we can see in fig.~\ref{fig:ma_corrs} there is a high correlation for most of the makes. Thus the cars we detect from street view images contain useful information about the types of cars driven by people in a particular zip code.

\begin{figure} [t]
\begin{center}
\includegraphics[width=1\linewidth]{img/boston_make_corr.png}
\end{center}
\caption {Pearson correlation coefficient and p values between the number of detected and registered cars in Boston for each make.}
\label{fig:ma_corrs}
\end{figure}


\begin{figure}[t]
\begin{center}
   \includegraphics[width=0.9\linewidth]{img/averagePriceIncome.png}
\end{center}
   \caption {Median household income per zip code vs average price of detected cars per zip code.}
\label{fig:price-income-corr}
\end{figure}

\subsubsection{What do rich/poor people drive?}
We gathered zip code level as well as census tract level 2007-2012 American Community Survey data for the 200 cities in our dataset and analyzed how the census data relates to statistics from our detected cars. 

Table \ref{table:car-census-corrs} shows correlation values between various attributes of the detected cars and median household income as well as education level per zip code. Fig.~\ref{fig:price-income-corr} shows a plot of median household income vs. average car price in a zip code. As expected, there is a high correlation between median household income and the average car price per zip code (r=0.49, p \(\sim\) 0). Our results also indicate that rich people prefer to drive foreign, especially German, cars (r=0.59). What is perhaps surprising is that there is a very high negative correlation (r=-0.55, p \(\sim\) 0) between the percentage of American cars in a zip code and median household income. So poor people live in places with many American cars.

Poor people also live near very old cars where as rich people live near newer ones. As table \ref{table:car-census-corrs} shows, the correlation between median household income and the number of cars in 1990-1994 is very negative and increases to a high positive 0.59 for cars in the 2005-2009 range. Finally, a perhaps not surprising result is that poor people live near cars with low miles per gallon (MPG). This corroborates~\cite{cal-traffic-study} study showing that poor people are more exposed to car pollution than rich people.
 
\begin{figure}[t]
\begin{center}
    \includegraphics[width=0.45\linewidth]{img/price.png}
    \includegraphics[width=0.45\linewidth]{img/income.png}
  \raisebox{-.5\height}{
    \includegraphics[width=0.45\linewidth]{img/houston_price.png}
    \includegraphics[width=0.45\linewidth]{img/houston_income.png}
  }
\end{center}
   \caption {Top left: Heat map of average car price in Boston, right: heat map of median household income in Boston. Bottom left: Heat map of average car price in Houston, right: heat map of median household income in Houston}
\label{fig:bos-sf-vis}
\end{figure}

\subsubsection{How does education relate to cars on the street?}
As shown in \ref{table:car-census-corrs} there is a high negative correlation between the number of people with only a high school education and the average price of a car in a zip code. As expected, we also found a high correlation between the number of college educated people in a zip code and the average car price. What is perhaps surprising is that although there is a large increase in correlation coefficient as we go from high school to college educated, the jump from college to graduate school is very low (r=0.31 for college educated and 0.39 for graduate school). This tells us that there is a very low difference in the price of cars driven by people who only hold bachelors as opposed to graduate degrees.

\begin{figure*}[t]
\begin{center}
  \raisebox{-.02\height}
 {
   \includegraphics[width=0.3\linewidth]{img/sf_density.png}
 }
   \includegraphics[width=0.3\linewidth]{img/sf_mpg.png}
   \includegraphics[width=0.3\linewidth]{img/sf_air_cropped.png}
\end{center}
   \caption {A. Density of cars in San francisco inversely weighted by their expected MPG, B. The weighted average of car MPG in San Francisco. The weights are the expected number of cars, C. Ground Truth for Air quality (measured in annual particulate matter) in San Francisco.}
\label{fig:pollution}
\end{figure*}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|r|}
\hline
\textbf{Census Variable} & \textbf{Car Attribute} & \textbf{r}  \\
\hline\hline
Household income & No. 1990-1994 cars & -0.42 \\
Household income & No. 1995-1999 cars & -0.40 \\
Household income & No. 2000-2004 cars & 0.21 \\
Household income & No. 2005-2009 cars & 0.46 \\
Household income & \% foreign cars & 0.59 \\
Household income & \% German cars & 0.57 \\
Household income & \% Us cars & -0.59 \\
Household income & Avg. price & 0.49 \\
Education: highschool & Avg. price & -0.21 \\
Education: college & Avg. price & 0.32 \\
Education: graduate scl. & Avg. price & 0.39 \\
\hline
\end{tabular}
\end{center}
\caption{Pearson correlation coefficient between various census variables and detected car attributes. All p values are \(\sim\) 0.}
\label{table:car-census-corrs}
\end{table}

\subsection{What cars on the street tell us about neighborhoods}
\subsubsection{Which neighborhoods are wealthy/poor?}
We ask the question: what can our street view car detections tell us about the wealth of a neighborhood? Specifically, can we predict which neighborhoods are wealthy or un-wealthy using our detections? Intuitively, if we see many expensive cars on the street, we suspect that we are in a rich neighborhood and vice versa. Figure \ref{fig:bos-sf-vis} A shows a heat map of the average price of detected cars within a zip code and median household income in a zip code for Boston and figure \ref{fig:bos-sf-vis} B shows the same visualization for Houston. We can see that in both cities, the average car price in a zip code is a very good predictor of wealthy/un-wealthy neighborhoods.


\subsubsection{Which neighborhoods have high car pollution?}
Can our street view detections tell us anything about which neighborhoods are affected by highly polluting cars? To answer this question, we plotted a heat map of the expected number of cars per sample inversely weighted by the expected MPG of that sample as well as the weighted average of car MPG where the weights are the expected number of cars. The first measure should give us a rough idea of the location of highly polluting neighborhoods: for the same density of cars, areas with high MPG result in lower numbers than those with low MPG. For different densities of cars, the relative magnitude of the measure depends on both the density of cars and how efficient they are. The second measure, on the other hand, visualizes areas with a high concentration of low MPG cars.

Fig.~\ref{fig:pollution}A shows the weighted density of cars in San Francisco and B shows the weighted MPG. Although we could not find ground truth data of car pollution, Fig.~\ref{fig:pollution}C is a map of San Francisco air quality measuring annual average particulate matter concentration (MPG) from all sources. To our surprise, their map seems to agree with Fig.~\ref{fig:pollution}B in most cases.


\subsection{What cars on the street tell us about cities}
\subsubsection{Which cities are more segregated?}
In this section we ask the question: which cities show high clustering of similarly priced cars? Specifically, which cities have expensive cars clustered together with other expensive cars and cheap cars clustered with other cheap cars? Given the high correlation between median household income and average car price, the answer to this question should give us a good indication of the cities that are most and least segregated. Following the analysis of ~\cite{mit_plos_1} we use the Moran I statistic to measure spatial autocorrelation where a value of 1 indicates perfect clustering of similar values, -1 indicates perfect dispersion and 0 indicates a random spatial arrangement (neither clustering nor dispersion). Fig.~\ref{fig:moran-i} plots the highest and lowest scoring cities as well as well as a few others in between. We can see that Reno shows the highest clustering where as Dover shows the lowest.

\begin{figure}[t]
\begin{center}
    \includegraphics[width=0.9\linewidth]{img/moran_i.pdf}
\end{center}
   \caption {Moran I scores for car prices. The highest and lowest scoring cities are shown as well as 5 cities with scores in between. Reno exhibits the most amount of clustering by car price where as Dover exhibits no clustering.} 
\label{fig:moran-i}
\end{figure}

\subsubsection{Which cities are more patriotic?}
Which cities have the most number of domestic cars? As Fig.~\ref{fig:city_price}A shows the coastal cities have a high concentration of foreign made cars where as the midwest has a low concentration. This result is to be expected since the coasts also have a higher number of immigrants as well as a higher concentration of wealthy people. And as we showed in our previous analyses, wealthy people tend to drive foreign cars. The city with the highest percentage of foreign cars was found to be San Francisco where as Casper Wyoming had the least percentage.

\subsubsection{Which cities are wealthier?}
In our final analysis, we ask which city has the most expensive cars on average? Fig.~\ref{fig:city_price}B maps the average car price for each city. We can see that many of the east coast cities have expensive cars as well as some cities in the south like Atlanta. We found the city with the most expensive cars to be New York and the one with the least expensive cars to be El Paso. The fact that our results predict New York may not be surprising given that some of the wealthiest people in the United States live there and those who are less wealthy tend to take public transportation.

\begin{figure*}[t]
\begin{center}
    \includegraphics[width=0.48\linewidth]{img/city_foreign_new.png}
    \includegraphics[width=0.50\linewidth]{img/city_price_new.png}
\end{center}
   \caption {A. Map of the percentage of foreign cars in each city. San Francisco has the highest percentage and Casper the lowest. B. Map of the United States showing the expected car price in each city. New York has the highest expected car price where as El Paso has the lowest.}
\label{fig:city_price}
\end{figure*}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Attribute} & \textbf{Acc\%} \\
\hline\hline
Make & X \\
Model & X \\
Submodel & X \\
Year (5 bins) & X \\
Price (5 bins)& X \\
Domestic/Foreign & X\\
Country & X\\
\hline
\end{tabular}
\end{center}
\caption{Gain in fine-grained accuracy with ground truth attributes.}
\label{table:ground}
\end{table}

\begin{table}
\begin{center}
\begin{tabular}{|l|c|c|r|}
\hline
\textbf{Attribute} & \textbf{Census Variable}& \textbf{acc-fg} &\textbf{acc-m}\\
\hline\hline
Price & Median hh income & X & X\\
Year  & Median hh income & X & X\\
Make & No. ppl in Management & X &X \\
Submodel & No. ppl 6-17 years & X & X \\
Domestic & Median hh income & X & X\\
Country & Median hh income & X & X\\
\hline
\end{tabular}
\end{center}
\caption{Census variables resulting in the highest accuracy gain for each car attribute. Acc-fg is the fine-grained accuracy and acc-m is the make level accuracy. Car price/year with median household income give the highest gain in accuracy.}
\label{table:prior-acc}
\end{table}

\section{Using social priors to improve classification}
\label{sec:prior}
In section~\ref{sec:social} we showed that street view car detections give a lot of information about people, neighborhoods and cities. In this section we ask the reverse question: does our knowledge of neighborhood demographics teach us something about the cars in that neighborhood? Intuitively, if we know that a particular image was taken in a wealthy neighborhood, for example, we would expect the cars in that neighborhood to be expensive. Can we use this information to improve our car classifications?

To answer this question, we first tried to use zip code level census variables as priors, calculating \(P(C|I,Sk)\) where \(C\) is the fine-grained class, \(I\) is an image and \(Sk\) \(\in\) \(\{\)\(S1\)\ldots \(Sn\)\(\}\)is a particular zip code level census variable such as median household income. After applying Bayes' rule, assuming that the image and census data are conditionally independent given the class label, and applying Bayes' rule again we get:

\begin{equation}
P(C|I,Sk)\propto \frac{P(C|I)}{P(C)}P(C|Sk)
\label{eq:prior-eq}
\end{equation}

Where \(P(C|I)\) is the output of our CNN classifier. A naive way of using census variables such as the one above reduces accuracy by 5.x\% to ~26.x\%. This is not surprising given that we have over 2600 fine-grained classes, and many of them have similar attributes such as price. We expect census variables to be good indicators of cheap vs. expensive or old vs. new cars but not to give us much information about the presence/absence of cars with similar attributes at the fine-grained level such as 2001 Honda Accord lx vs. 2001 Honda Accord dx. With this insight, instead of using census variables directly as fine-grained class priors, we use them as priors for the car attributes. Table~\ref{table:ground} lists fine-grained accuracy gains after plugging in ground truth attributes. These values give an upper bound for the gain in accuracy that can be achieved after perfectly predicting each attribute. With this in mind, to incorporate the census variables into our classification pipeline we can reformulate \(P(C|Sk)\) in equation~\ref{eq:prior-eq} as \(P(C|Aj)\)\(P(Aj|Sk)\) where \(Aj\) \(\in\) \(\{\)\(A1\)\ldots\(An\)\(\}\) represents a car attribute such as price. After this modification, equation~\ref{eq:prior-eq} can be written as  

\begin{equation}
  P(C|I,Sk) \propto \frac{P(C|I)}{P(C)}P(C|Aj)P(Aj|Sk)
\end{equation}
We calculate \(P(C|I,Sk)\) for all car attributes and 30 different census variables, quantizing some car attributes and census variables such as car price and median household income into bins ranging from 2-20. Table~\ref{table:prior-acc} shows the highest accuracy numbers for various combinations of census variables and car attributes. It can be seen that using median household income and either car price or year give the highest accuracy gain. This result is to be expected since, as seen in section~\ref{sec:social}, there is a high correlation between median household income and car price and year. Although the accuracy gain at the fine-grained level is very slight (which is to be expected due to the large number of similar classes in our dataset), we get more significant gains in accuracy at higher levels of the hierarchy such as car make. 

\section{Conclusion}
In conclusion, by analyzing car detections from 45 million images across 200 cities, we have shown that cars detected from street view images can teach us a lot about our neighborhoods, cities and their demographic makeup. To facilitate this work, we have collected the largest and most challenging fine-grained dataset reported to date. Finally, we showed that demographic knowledge can also help improve fine-grained classification. For our future work we plan to perform even more extensive social analysis\textemdash such as crime prediction and predicting changes in cities across time\textemdash using fine-grained detection. We also hope to further explore the use of demographic data to assist in fine-grained detection. 
{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}

{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}
\end{document}

 
